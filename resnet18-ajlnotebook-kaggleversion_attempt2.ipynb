{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":90489,"databundleVersionId":10898385,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# directly install the bttai files, will be used later for kaggle submission \n# REFERENCED THIS CODE FOR MODEL https://github.com/deepmancer/resnet-cifar-classification/blob/master/ResNet-18/CIFAR10_Classification.ipynb\n!pip install kaggle\n!kaggle competitions download bttai-ajl-2025","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:55:36.566865Z","iopub.execute_input":"2025-03-22T07:55:36.567262Z","iopub.status.idle":"2025-03-22T07:55:41.618975Z","shell.execute_reply.started":"2025-03-22T07:55:36.567234Z","shell.execute_reply":"2025-03-22T07:55:41.617804Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\nRequirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.17.0)\nRequirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.12.14)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.67.1)\nRequirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\nRequirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\nRequirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\nTraceback (most recent call last):\n  File \"/usr/local/bin/kaggle\", line 5, in <module>\n    from kaggle.cli import main\n  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 7, in <module>\n    api.authenticate()\n  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 407, in authenticate\n    raise IOError('Could not find {}. Make sure it\\'s located in'\nOSError: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"## Note - About file path\n\nYou could use the cell above to print the names of the file directories and get the following directories:\n\n```\n/kaggle/input/bttai-ajl-2025/sample_submission.csv\n/kaggle/input/bttai-ajl-2025/train.csv\n/kaggle/input/bttai-ajl-2025/test.csv\n/kaggle/input/bttai-ajl-2025/test/test/e0374ae6c1362ff183cfba28ded5421b.jpg\n/kaggle/input/bttai-ajl-2025/test/test/437159c605260bdd079af230566af291.jpg\n...\n...\n/kaggle/input/bttai-ajl-2025/train/train/dermatomyositis/11271bdf2598afdd4260db3125e1f6a5.jpg\n/kaggle/input/bttai-ajl-2025/train/train/dermatomyositis/732819951dcf2b53d15ea7b8bb123b71.jpg\n/kaggle/input/bttai-ajl-2025/train/train/dermatomyositis/6dcc7a8abb5e1c6e670101f4b6231246.jpg\n/kaggle/input/bttai-ajl-2025/train/train/dermatomyositis/e63c3b3f0ab8905e204fe467cc7411f9.jpg\n...\n...\n```\n\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Import Necessary Libraries","metadata":{}},{"cell_type":"code","source":"# 1. Import Necessary Libraries\nimport pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport keras\nfrom keras import Sequential\nfrom keras import layers\nfrom keras.layers import Conv2D,MaxPool2D,Dense,Flatten,BatchNormalization,Dropout, Input\nfrom keras.optimizers import Adam\nfrom PIL import Image\nfrom tensorflow import data as tf_data\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing import image\n\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms,models,datasets\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\n\nfrom torch import optim\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nfrom glob import glob\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport torch.utils.data as data\nimport random\nimport warnings\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\n\nfrom sklearn import decomposition\nfrom sklearn import manifold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nimport copy\nfrom collections import namedtuple\nimport os\nimport random\nimport shutil\nimport time\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n\nmodel.eval()\n# or any of these variants\n# Explanation:\n# - pandas and numpy: for data manipulation\n# - sklearn: for splitting data and encoding labels\n# - tensorflow.keras: for building and training the neural network\nfrom sklearn.manifold import TSNE\nfrom torch import linalg as LA","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:55:42.312476Z","iopub.execute_input":"2025-03-22T07:55:42.312848Z","iopub.status.idle":"2025-03-22T07:55:42.609343Z","shell.execute_reply.started":"2025-03-22T07:55:42.312820Z","shell.execute_reply":"2025-03-22T07:55:42.608205Z"}},"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"print(device)\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:55:44.412752Z","iopub.execute_input":"2025-03-22T07:55:44.413156Z","iopub.status.idle":"2025-03-22T07:55:44.418760Z","shell.execute_reply.started":"2025-03-22T07:55:44.413125Z","shell.execute_reply":"2025-03-22T07:55:44.417696Z"}},"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"## 2. Load Data\n\nMake sure to verify the file paths if you're running on a different platform.","metadata":{}},{"cell_type":"code","source":"# 2. Load Data\ntrain_df = pd.read_csv('/kaggle/input/bttai-ajl-2025/train.csv')\ntest_df = pd.read_csv('/kaggle/input/bttai-ajl-2025/test.csv')\n\n# Add .jpg extension to md5hash column to reference the file_name\ntrain_df['md5hash'] = train_df['md5hash'].astype(str) + '.jpg'\ntest_df['md5hash'] = test_df['md5hash'].astype(str) + '.jpg'\n\n# Combine label and md5hash to form the correct path\ntrain_df['file_path'] = train_df['label'] + '/' + train_df['md5hash']\n\n\n# ALSO TRAIN AND TEST DIRECTORIES \n# need to change to train_dir = '/kaggle/input/bttai-ajl-2025/train/train/' for kaggle\ntrain_dir = '/kaggle/input/bttai-ajl-2025/train/train'\ntest_dir = '/kaggle/input/bttai-ajl-2025/test/test'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:55:45.057806Z","iopub.execute_input":"2025-03-22T07:55:45.058214Z","iopub.status.idle":"2025-03-22T07:55:45.080033Z","shell.execute_reply.started":"2025-03-22T07:55:45.058179Z","shell.execute_reply":"2025-03-22T07:55:45.078979Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# Check the first few rows to understand the structure\ntrain_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:55:46.436951Z","iopub.execute_input":"2025-03-22T07:55:46.437346Z","iopub.status.idle":"2025-03-22T07:55:46.450551Z","shell.execute_reply.started":"2025-03-22T07:55:46.437318Z","shell.execute_reply":"2025-03-22T07:55:46.449574Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"                                md5hash  fitzpatrick_scale  \\\n0  fd06d13de341cc75ad679916c5d7e6a6.jpg                  4   \n1  a4bb4e5206c4e89a303f470576fc5253.jpg                  1   \n2  c94ce27e389f96bda998e7c3fa5c4a2e.jpg                  5   \n3  ebcf2b50dd943c700d4e2b586fcd4425.jpg                  3   \n4  c77d6c895f05fea73a8f3704307036c0.jpg                  1   \n\n   fitzpatrick_centaur                             label nine_partition_label  \\\n0                    4                 prurigo-nodularis     benign-epidermal   \n1                    1  basal-cell-carcinoma-morpheiform  malignant-epidermal   \n2                    5                            keloid         inflammatory   \n3                    3              basal-cell-carcinoma  malignant-epidermal   \n4                    1                 prurigo-nodularis     benign-epidermal   \n\n  three_partition_label            qc  ddi_scale  \\\n0                benign           NaN         34   \n1             malignant           NaN         12   \n2        non-neoplastic  1 Diagnostic         56   \n3             malignant           NaN         34   \n4                benign           NaN         12   \n\n                                           file_path  \n0  prurigo-nodularis/fd06d13de341cc75ad679916c5d7...  \n1  basal-cell-carcinoma-morpheiform/a4bb4e5206c4e...  \n2        keloid/c94ce27e389f96bda998e7c3fa5c4a2e.jpg  \n3  basal-cell-carcinoma/ebcf2b50dd943c700d4e2b586...  \n4  prurigo-nodularis/c77d6c895f05fea73a8f37043070...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>md5hash</th>\n      <th>fitzpatrick_scale</th>\n      <th>fitzpatrick_centaur</th>\n      <th>label</th>\n      <th>nine_partition_label</th>\n      <th>three_partition_label</th>\n      <th>qc</th>\n      <th>ddi_scale</th>\n      <th>file_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fd06d13de341cc75ad679916c5d7e6a6.jpg</td>\n      <td>4</td>\n      <td>4</td>\n      <td>prurigo-nodularis</td>\n      <td>benign-epidermal</td>\n      <td>benign</td>\n      <td>NaN</td>\n      <td>34</td>\n      <td>prurigo-nodularis/fd06d13de341cc75ad679916c5d7...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a4bb4e5206c4e89a303f470576fc5253.jpg</td>\n      <td>1</td>\n      <td>1</td>\n      <td>basal-cell-carcinoma-morpheiform</td>\n      <td>malignant-epidermal</td>\n      <td>malignant</td>\n      <td>NaN</td>\n      <td>12</td>\n      <td>basal-cell-carcinoma-morpheiform/a4bb4e5206c4e...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>c94ce27e389f96bda998e7c3fa5c4a2e.jpg</td>\n      <td>5</td>\n      <td>5</td>\n      <td>keloid</td>\n      <td>inflammatory</td>\n      <td>non-neoplastic</td>\n      <td>1 Diagnostic</td>\n      <td>56</td>\n      <td>keloid/c94ce27e389f96bda998e7c3fa5c4a2e.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ebcf2b50dd943c700d4e2b586fcd4425.jpg</td>\n      <td>3</td>\n      <td>3</td>\n      <td>basal-cell-carcinoma</td>\n      <td>malignant-epidermal</td>\n      <td>malignant</td>\n      <td>NaN</td>\n      <td>34</td>\n      <td>basal-cell-carcinoma/ebcf2b50dd943c700d4e2b586...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>c77d6c895f05fea73a8f3704307036c0.jpg</td>\n      <td>1</td>\n      <td>1</td>\n      <td>prurigo-nodularis</td>\n      <td>benign-epidermal</td>\n      <td>benign</td>\n      <td>NaN</td>\n      <td>12</td>\n      <td>prurigo-nodularis/c77d6c895f05fea73a8f37043070...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":53},{"cell_type":"markdown","source":"## 3. Data Preprocessing\n\n\nThis section demonstrates basic preprocessing techniques. To enhance data quality and model performance, consider incorporating more advanced preprocessing methods.\n\nFor further guidance, feel free to take a look at the [Image Preprocessing tutorial](https://colab.research.google.com/drive/1-ItNcRMbZBE6BCwPT-wD8m3YmHqwHxme?usp=sharing)  available in the 'Resources' section of this Kaggle competition.\n","metadata":{}},{"cell_type":"code","source":"# Drop DDI and qc for null values cols \n\ntrain_df = train_df.drop('ddi_scale', axis=1)\ntrain_df = train_df.drop('qc', axis=1)\n# Display\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:55:48.047660Z","iopub.execute_input":"2025-03-22T07:55:48.048032Z","iopub.status.idle":"2025-03-22T07:55:48.054676Z","shell.execute_reply.started":"2025-03-22T07:55:48.048003Z","shell.execute_reply":"2025-03-22T07:55:48.053698Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"train_df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:55:48.268021Z","iopub.execute_input":"2025-03-22T07:55:48.268373Z","iopub.status.idle":"2025-03-22T07:55:48.280870Z","shell.execute_reply.started":"2025-03-22T07:55:48.268346Z","shell.execute_reply":"2025-03-22T07:55:48.279801Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"                                md5hash  fitzpatrick_scale  \\\n0  fd06d13de341cc75ad679916c5d7e6a6.jpg                  4   \n1  a4bb4e5206c4e89a303f470576fc5253.jpg                  1   \n2  c94ce27e389f96bda998e7c3fa5c4a2e.jpg                  5   \n3  ebcf2b50dd943c700d4e2b586fcd4425.jpg                  3   \n4  c77d6c895f05fea73a8f3704307036c0.jpg                  1   \n5  9d5a90fa3f6934608add10e698001760.jpg                  3   \n6  57885e3f5a3c043c3621a06bca196282.jpg                  2   \n7  8adbbbc4e50a0df8b89710dfd495d3c5.jpg                  5   \n8  763ed484fcc50bf7b67cc44f95bac95e.jpg                 -1   \n9  0198c74d604fde7055671d1b35869664.jpg                  3   \n\n   fitzpatrick_centaur                             label nine_partition_label  \\\n0                    4                 prurigo-nodularis     benign-epidermal   \n1                    1  basal-cell-carcinoma-morpheiform  malignant-epidermal   \n2                    5                            keloid         inflammatory   \n3                    3              basal-cell-carcinoma  malignant-epidermal   \n4                    1                 prurigo-nodularis     benign-epidermal   \n5                    5                 prurigo-nodularis     benign-epidermal   \n6                    1              seborrheic-keratosis     benign-epidermal   \n7                    4                            eczema         inflammatory   \n8                   -1                      folliculitis         inflammatory   \n9                    4           squamous-cell-carcinoma  malignant-epidermal   \n\n  three_partition_label                                          file_path  \n0                benign  prurigo-nodularis/fd06d13de341cc75ad679916c5d7...  \n1             malignant  basal-cell-carcinoma-morpheiform/a4bb4e5206c4e...  \n2        non-neoplastic        keloid/c94ce27e389f96bda998e7c3fa5c4a2e.jpg  \n3             malignant  basal-cell-carcinoma/ebcf2b50dd943c700d4e2b586...  \n4                benign  prurigo-nodularis/c77d6c895f05fea73a8f37043070...  \n5                benign  prurigo-nodularis/9d5a90fa3f6934608add10e69800...  \n6                benign  seborrheic-keratosis/57885e3f5a3c043c3621a06bc...  \n7        non-neoplastic        eczema/8adbbbc4e50a0df8b89710dfd495d3c5.jpg  \n8        non-neoplastic  folliculitis/763ed484fcc50bf7b67cc44f95bac95e.jpg  \n9             malignant  squamous-cell-carcinoma/0198c74d604fde7055671d...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>md5hash</th>\n      <th>fitzpatrick_scale</th>\n      <th>fitzpatrick_centaur</th>\n      <th>label</th>\n      <th>nine_partition_label</th>\n      <th>three_partition_label</th>\n      <th>file_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fd06d13de341cc75ad679916c5d7e6a6.jpg</td>\n      <td>4</td>\n      <td>4</td>\n      <td>prurigo-nodularis</td>\n      <td>benign-epidermal</td>\n      <td>benign</td>\n      <td>prurigo-nodularis/fd06d13de341cc75ad679916c5d7...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a4bb4e5206c4e89a303f470576fc5253.jpg</td>\n      <td>1</td>\n      <td>1</td>\n      <td>basal-cell-carcinoma-morpheiform</td>\n      <td>malignant-epidermal</td>\n      <td>malignant</td>\n      <td>basal-cell-carcinoma-morpheiform/a4bb4e5206c4e...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>c94ce27e389f96bda998e7c3fa5c4a2e.jpg</td>\n      <td>5</td>\n      <td>5</td>\n      <td>keloid</td>\n      <td>inflammatory</td>\n      <td>non-neoplastic</td>\n      <td>keloid/c94ce27e389f96bda998e7c3fa5c4a2e.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ebcf2b50dd943c700d4e2b586fcd4425.jpg</td>\n      <td>3</td>\n      <td>3</td>\n      <td>basal-cell-carcinoma</td>\n      <td>malignant-epidermal</td>\n      <td>malignant</td>\n      <td>basal-cell-carcinoma/ebcf2b50dd943c700d4e2b586...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>c77d6c895f05fea73a8f3704307036c0.jpg</td>\n      <td>1</td>\n      <td>1</td>\n      <td>prurigo-nodularis</td>\n      <td>benign-epidermal</td>\n      <td>benign</td>\n      <td>prurigo-nodularis/c77d6c895f05fea73a8f37043070...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>9d5a90fa3f6934608add10e698001760.jpg</td>\n      <td>3</td>\n      <td>5</td>\n      <td>prurigo-nodularis</td>\n      <td>benign-epidermal</td>\n      <td>benign</td>\n      <td>prurigo-nodularis/9d5a90fa3f6934608add10e69800...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>57885e3f5a3c043c3621a06bca196282.jpg</td>\n      <td>2</td>\n      <td>1</td>\n      <td>seborrheic-keratosis</td>\n      <td>benign-epidermal</td>\n      <td>benign</td>\n      <td>seborrheic-keratosis/57885e3f5a3c043c3621a06bc...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8adbbbc4e50a0df8b89710dfd495d3c5.jpg</td>\n      <td>5</td>\n      <td>4</td>\n      <td>eczema</td>\n      <td>inflammatory</td>\n      <td>non-neoplastic</td>\n      <td>eczema/8adbbbc4e50a0df8b89710dfd495d3c5.jpg</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>763ed484fcc50bf7b67cc44f95bac95e.jpg</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>folliculitis</td>\n      <td>inflammatory</td>\n      <td>non-neoplastic</td>\n      <td>folliculitis/763ed484fcc50bf7b67cc44f95bac95e.jpg</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0198c74d604fde7055671d1b35869664.jpg</td>\n      <td>3</td>\n      <td>4</td>\n      <td>squamous-cell-carcinoma</td>\n      <td>malignant-epidermal</td>\n      <td>malignant</td>\n      <td>squamous-cell-carcinoma/0198c74d604fde7055671d...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"# visualize images # TRIGGER WARNING THE PICS ARE KINDA GRAPHIC!!!!\ntrain_ds, val_ds = keras.utils.image_dataset_from_directory(\n    \"/kaggle/input/bttai-ajl-2025/train/train\",             # Path to the directory containing the dataset.\n    validation_split=0.2,    # Specify that 20% of the dataset will be used for validation.\n    subset=\"both\",           # Create both training and validation datasets from the directory.\n    seed=42,                 # Random seed to ensure reproducibility when shuffling the data.\n    #image_size=image_size,   # Resize all images to the specified dimensions (180x180).\n    batch_size=32,   # Divide the dataset into batches of the specified size (32 images per batch).\n)\n\n'''\n# commented section bc images I do not want to see them T_T \nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(np.array(images[i]).astype(\"uint8\"))\n        plt.title(int(labels[i]))\n        plt.axis(\"off\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:55:49.491641Z","iopub.execute_input":"2025-03-22T07:55:49.492030Z","iopub.status.idle":"2025-03-22T07:55:50.360394Z","shell.execute_reply.started":"2025-03-22T07:55:49.492001Z","shell.execute_reply":"2025-03-22T07:55:50.359496Z"}},"outputs":[{"name":"stdout","text":"Found 2860 files belonging to 21 classes.\nUsing 2288 files for training.\nUsing 572 files for validation.\n","output_type":"stream"},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"'\\n# commented section bc images I do not want to see them T_T \\nplt.figure(figsize=(10, 10))\\nfor images, labels in train_ds.take(1):\\n    for i in range(9):\\n        ax = plt.subplot(3, 3, i + 1)\\n        plt.imshow(np.array(images[i]).astype(\"uint8\"))\\n        plt.title(int(labels[i]))\\n        plt.axis(\"off\")\\n'"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"def create_generator(dataframe, directory, batch_size=32, target_size=(128, 128)):\n    \"\"\"\n   we didnt modify this function all that much \n    \"\"\"\n    # Fill in the correct flow_from_dataframe parameters\n    generator = train_datagen.flow_from_dataframe(\n        dataframe=dataframe,\n        directory=directory,\n        x_col='file_path',  # Use combined path\n        y_col='encoded_label',\n        target_size=target_size,\n        batch_size=batch_size,\n        class_mode='raw',\n        validate_filenames=False  # Disable strict filename validation\n    )\n    return generator\n\n\n# create train data \n# preprocess - from btt ai notebook sample for preprocessing the data for the model \nlabel_encoder = LabelEncoder()\ntrain_df['encoded_label'] = label_encoder.fit_transform(train_df['label'])\n\n# split the data into training and validation sets\ntrain_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)\n\n# define image data generators for training and validation\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nval_datagen = ImageDataGenerator(rescale=1./255)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:55:52.691483Z","iopub.execute_input":"2025-03-22T07:55:52.691827Z","iopub.status.idle":"2025-03-22T07:55:52.703356Z","shell.execute_reply.started":"2025-03-22T07:55:52.691802Z","shell.execute_reply":"2025-03-22T07:55:52.702410Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# encode data \nencoder1 = LabelEncoder()\ntrain_df['encoded_label'] = label_encoder.fit_transform(train_df['label'])\ntrain_data, val_data = train_test_split(train_df, test_size=0.2, random_state = 42)\n\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nval_datagen = ImageDataGenerator(rescale=1./255)\n\n\n# Create generators\ntrain_generator = create_generator(train_data, train_dir)\nval_generator = create_generator(val_data, train_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:55:52.905355Z","iopub.execute_input":"2025-03-22T07:55:52.905699Z","iopub.status.idle":"2025-03-22T07:55:52.924794Z","shell.execute_reply.started":"2025-03-22T07:55:52.905673Z","shell.execute_reply":"2025-03-22T07:55:52.923876Z"}},"outputs":[{"name":"stdout","text":"Found 2288 non-validated image filenames.\nFound 572 non-validated image filenames.\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"train_data = train_dir\ntrain_data.__len__()\ntest_data = test_dir\ntest_data.__len__()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:55:54.567619Z","iopub.execute_input":"2025-03-22T07:55:54.567994Z","iopub.status.idle":"2025-03-22T07:55:54.574461Z","shell.execute_reply.started":"2025-03-22T07:55:54.567966Z","shell.execute_reply":"2025-03-22T07:55:54.573458Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"38"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# use pytorch to finish the preprocessing \nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\n\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, directory, transform=None):\n        self.dataframe = dataframe\n        self.directory = directory\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.directory, self.dataframe.iloc[idx, 0])\n        image = Image.open(img_name).convert(\"RGB\")\n        label = self.dataframe.iloc[idx, 1]  # Adjust according to your labels\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard ImageNet normalization\n])\n\ndef create_loader(dataframe, directory, batch_size=32, transform=None):\n    dataset = CustomDataset(dataframe, directory, transform)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    return loader\n\n# Create data loaders for training and validation\ntrain_loader = create_loader(train_data, train_dir, batch_size=32, transform=transform)\nval_loader = create_loader(val_data, train_dir, batch_size=32, transform=transform)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:55:56.737276Z","iopub.execute_input":"2025-03-22T07:55:56.737672Z","iopub.status.idle":"2025-03-22T07:55:56.744164Z","shell.execute_reply.started":"2025-03-22T07:55:56.737640Z","shell.execute_reply":"2025-03-22T07:55:56.743218Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"'\\n# use pytorch to finish the preprocessing \\nfrom torch.utils.data import Dataset, DataLoader\\nfrom torchvision import transforms\\nfrom PIL import Image\\nimport os\\n\\nclass CustomDataset(Dataset):\\n    def __init__(self, dataframe, directory, transform=None):\\n        self.dataframe = dataframe\\n        self.directory = directory\\n        self.transform = transform\\n\\n    def __len__(self):\\n        return len(self.dataframe)\\n\\n    def __getitem__(self, idx):\\n        img_name = os.path.join(self.directory, self.dataframe.iloc[idx, 0])\\n        image = Image.open(img_name).convert(\"RGB\")\\n        label = self.dataframe.iloc[idx, 1]  # Adjust according to your labels\\n\\n        if self.transform:\\n            image = self.transform(image)\\n\\n        return image, label\\n\\ntransform = transforms.Compose([\\n    transforms.Resize((128, 128)),\\n    transforms.ToTensor(),\\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard ImageNet normalization\\n])\\n\\ndef create_loader(dataframe, directory, batch_size=32, transform=None):\\n    dataset = CustomDataset(dataframe, directory, transform)\\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\\n    return loader\\n\\n# Create data loaders for training and validation\\ntrain_loader = create_loader(train_data, train_dir, batch_size=32, transform=transform)\\nval_loader = create_loader(val_data, train_dir, batch_size=32, transform=transform)\\n'"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"def create_loader(dataframe, directory, batch_size=32, transform=None):\n    dataset = CustomDataset(dataframe, directory, transform)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    return loader\n\n# Create data loaders for training and validation\ntrain_loader = create_loader(train_data, train_dir, batch_size=32, transform=transform)\nval_loader = create_loader(val_data, train_dir, batch_size=32, transform=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:55:56.925005Z","iopub.execute_input":"2025-03-22T07:55:56.925376Z","iopub.status.idle":"2025-03-22T07:55:56.931321Z","shell.execute_reply.started":"2025-03-22T07:55:56.925348Z","shell.execute_reply":"2025-03-22T07:55:56.929907Z"}},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"## 4. Build the model\n","metadata":{}},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, in_len, out_len, stride=1):\n        super(ResidualBlock, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_len, out_len, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_len)\n        self.conv2 = nn.Conv2d(out_len, out_len, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_len)\n        \n        layers = []\n        if stride != 1 or in_len != out_len:\n            layers.append(nn.Conv2d(in_len, out_len, kernel_size=1, stride=stride, bias=False))\n            layers.append(nn.BatchNorm2d(out_len))\n            self.shortcut = nn.Sequential(*layers)\n\n        else:\n            self.shortcut = nn.Sequential()\n        \n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n        out = self.bn2(out)\n        out = self.conv2(out)\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n    \n    \nclass ResNet18(nn.Module):\n    def __init__(self):\n        super(ResNet18, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n                \n        self.layer1 = nn.Sequential(\n            ResidualBlock(64, 64, stride=1),\n            ResidualBlock(64, 64, stride=1)\n        )\n        self.layer2 = nn.Sequential(\n            ResidualBlock(64, 128, stride=2),\n            ResidualBlock(128, 128, stride=1)\n        )\n        self.layer3 = nn.Sequential(\n            ResidualBlock(128, 256, stride=2),\n            ResidualBlock(256, 256, stride=1)\n        )\n        \n        self.layer4 = nn.Sequential(\n            ResidualBlock(256, 512, stride=2),\n            ResidualBlock(512, 512, stride=1)\n        )\n        \n        self.linear = nn.Linear(512, 10)\n\n    def create_block(self, out_len, stride):\n        layers = []\n        layers.append(ResidualBlock(self.in_len, out_len, stride=stride))\n        self.in_len = out_len\n        layers.append(ResidualBlock(self.in_len, out_len, stride=1))\n        return nn.Sequential(*layers)\n    \n    def get_feature_space(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        return out\n    \n    def forward(self, x):\n        out = self.get_feature_space(x)\n        out = self.linear(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:55:58.766027Z","iopub.execute_input":"2025-03-22T07:55:58.766436Z","iopub.status.idle":"2025-03-22T07:55:58.780546Z","shell.execute_reply.started":"2025-03-22T07:55:58.766408Z","shell.execute_reply":"2025-03-22T07:55:58.779501Z"}},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":"## 5. Train the Model\n","metadata":{}},{"cell_type":"code","source":"# TODO: Train your model here. \n#https://www.kaggle.com/code/ggsri123/implementing-resnet18-for-image-classification\ndef train_model(model, train_loader, val_loader, criterion, optimizer, lr_scheduler, num_epochs=50):\n    best_accuracy = 0.0\n    val_acc_history = []\n    train_acc_history = []\n    val_loss_history = []\n    train_loss_history = []\n\n    steps = ['train', 'validation']\n    dataloaders = {'train': train_loader, 'validation': val_loader}\n\n    print(f\"Number of epochs = {num_epochs}\")\n    print('*********************')\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n\n        for step in steps:\n            print(f\"Step = {step}\")\n            pre_epoch(step, model)\n            \n            # Initialize accumulators\n            running_loss = 0.0\n            correct_number = 0\n            total = 0\n\n            # Iterate through batches\n            for data, targets in dataloaders[step]:\n                data, targets = data.to(device), targets.to(device)\n                optimizer.zero_grad()\n\n                # Forward pass\n                outputs = model(data)\n                batch_loss = criterion(outputs, targets)  # Rename loss to batch_loss\n                running_loss += batch_loss.item() * data.size(0)  # Accumulate loss\n                \n                _, preds = torch.max(outputs, 1)\n                correct_number += torch.sum(preds == targets.data)\n                total += targets.size(0)\n\n                # Backward pass for training step\n                if step == 'train':\n                    batch_loss.backward()\n                    optimizer.step()\n\n            epoch_loss = running_loss / total\n            epoch_accuracy = correct_number.double() / total\n\n            if step == 'validation':\n                lr_scheduler.step()\n\n                if epoch_accuracy > best_accuracy:\n                    best_accuracy = epoch_accuracy\n                    torch.save(model.state_dict(), 'best_model.pth')\n\n                val_acc_history.append(epoch_accuracy)\n                val_loss_history.append(epoch_loss)\n\n            if step == 'train':\n                train_acc_history.append(epoch_accuracy)\n                train_loss_history.append(epoch_loss)\n\n            print(f\"{step.capitalize()} Loss: {epoch_loss:.4f} Accuracy: {epoch_accuracy:.4f}\")\n\n        print('*********************')\n\n    return train_acc_history, val_acc_history, train_loss_history, val_loss_history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:56:00.911676Z","iopub.execute_input":"2025-03-22T07:56:00.912051Z","iopub.status.idle":"2025-03-22T07:56:00.921796Z","shell.execute_reply.started":"2025-03-22T07:56:00.912025Z","shell.execute_reply":"2025-03-22T07:56:00.920826Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"model = ResNet18()\nmodel.to(device)\n\n# this will instantiate the model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:56:02.815783Z","iopub.execute_input":"2025-03-22T07:56:02.816183Z","iopub.status.idle":"2025-03-22T07:56:02.922926Z","shell.execute_reply.started":"2025-03-22T07:56:02.816154Z","shell.execute_reply":"2025-03-22T07:56:02.921920Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"ResNet18(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer1): Sequential(\n    (0): ResidualBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (1): ResidualBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer2): Sequential(\n    (0): ResidualBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): ResidualBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer3): Sequential(\n    (0): ResidualBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): ResidualBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer4): Sequential(\n    (0): ResidualBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): ResidualBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (linear): Linear(in_features=512, out_features=10, bias=True)\n)"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"# optimize model \noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.000125)\ncriterion = nn.CrossEntropyLoss()\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:56:04.642353Z","iopub.execute_input":"2025-03-22T07:56:04.642715Z","iopub.status.idle":"2025-03-22T07:56:04.648364Z","shell.execute_reply.started":"2025-03-22T07:56:04.642687Z","shell.execute_reply":"2025-03-22T07:56:04.647350Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"from torchvision import transforms\n# Define transformations for training\ntrain_transform = transforms.Compose([\n    transforms.Resize((128, 128)),  # Resize images to 128x128\n    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n    transforms.ToTensor(),  # Convert PIL image to a tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean/std\n])\n\n# Define transformations for validation (no augmentation)\nval_transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, directory, transform=None):\n        self.dataframe = dataframe  # Ensure this is a DataFrame\n        self.directory = directory\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.directory, self.dataframe.iloc[idx, 0])  # Path\n        image = Image.open(img_name).convert(\"RGB\")\n        label = self.dataframe.iloc[idx, 1]  # Label\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\ntrain_dataset = CustomDataset(dataframe=train_df, directory=train_dir, transform=train_transform)\nval_dataset = CustomDataset(dataframe=val_data, directory=train_dir, transform=val_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:56:05.808749Z","iopub.execute_input":"2025-03-22T07:56:05.809127Z","iopub.status.idle":"2025-03-22T07:56:05.819414Z","shell.execute_reply.started":"2025-03-22T07:56:05.809099Z","shell.execute_reply":"2025-03-22T07:56:05.818384Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"def pre_epoch(step, model):\n    if step == 'train':\n        model.train()\n    elif step == 'validation':\n        model.eval()\n    else:\n        raise ValueError(f\"Unknown step: {step}. Expected 'train' or 'validation'.\")\n        \ntorch.cuda.empty_cache()\n\nnum_epochs = 50\n\nval_acc_history = []\ntrain_acc_history = []\nval_loss_history = []\ntrain_loss_history = []\n\ntorch.cuda.empty_cache()\nbest_accuracy = 0.0\nsteps = ['train', 'validation']\ndataloaders = {\n    'validation': val_loader,\n    'train': train_loader\n}\n\nprint('number of epochs = {}'.format(num_epochs))\nprint('*********************')\nfor epoch in range(num_epochs):\n    print('epoch = {}'.format(epoch))\n    for step in steps: \n        print('step = {}'.format(step))\n        pre_epoch(step, model)\n        loss = 0.0\n        correct_number = 0\n\n        for data, targets in dataloaders[step]: \n            data = data.to(device)\n            targets = targets.to(device)\n            optimizer.zero_grad()\n            with torch.set_grad_enabled(step == 'train'):\n                outputs = model(data)\n                loss = criterion(outputs, targets)\n                _, preds = torch.max(outputs, 1)\n                if step == 'train': \n                    loss.backward()\n                    optimizer.step()\n                    \n            loss += loss.item() * data.size(0)\n            correct_number += torch.sum(preds == targets.data)\n\n        if step == 'validation':\n            lr_scheduler.step()\n            \n        epoch_loss = loss / len(dataloaders[step].dataset)\n        epoch_accuracy = correct_number.double() / len(dataloaders[step].dataset)\n\n\n        if step == 'validation':\n            if epoch_accuracy > best_accuracy:\n                best_accuracy = epoch_accuracy\n                torch.save(model.state_dict(), 'cifar10-resnet18.pth')\n            val_acc_history.append(epoch_accuracy)\n            val_loss_history.append(epoch_loss)               \n            \n        if step == 'train':\n            train_acc_history.append(epoch_accuracy)\n            train_loss_history.append(epoch_loss)\n        \n        print('loss = {:.4f} accuracy = {:.4f}'.format(epoch_loss, epoch_accuracy))\n\n    print('*********************')\n\nval_acc_history = [x.item() for x in val_acc_history]\ntrain_acc_history = [x.item() for x in train_acc_history]\nval_loss_history = [x.item() for x in val_loss_history]\ntrain_loss_history = [x.item() for x in train_loss_history]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:56:06.956117Z","iopub.execute_input":"2025-03-22T07:56:06.956467Z","iopub.status.idle":"2025-03-22T07:56:07.032290Z","shell.execute_reply.started":"2025-03-22T07:56:06.956423Z","shell.execute_reply":"2025-03-22T07:56:07.030904Z"}},"outputs":[{"name":"stdout","text":"number of epochs = 50\n*********************\nepoch = 0\nstep = train\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-422012064f6c>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mcorrect_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-66-82e59f18f457>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3468\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3469\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3470\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3471\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/bttai-ajl-2025/train/train/4ff4109275ef98c7cd754dee47249adc.jpg'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/bttai-ajl-2025/train/train/4ff4109275ef98c7cd754dee47249adc.jpg'","output_type":"error"}],"execution_count":67},{"cell_type":"markdown","source":"## 6. Make Predictions on Test Data","metadata":{}},{"cell_type":"code","source":"# 6. Make Predictions on Test Data\ndef preprocess_test_data(test_df, directory):\n    # TODO: create a generator for the test set here.\n    # test_datagen = ImageDataGenerator(rescale=1./255)\n    # test_generator = test_datagen.flow_from_dataframe(\n    #     .... ## set the test_generator here \n    # )\n    # return test_generator\n    \"\"\"\n    Template for loading and preprocessing test images with optional transforms.\n    Accepts the 'transform' argument to preprocess images.\n    \"\"\"\n    test_loader = create_loader(test_df, directory, batch_size=32, transform=transform)\n    model.eval()\n    predictions = []\n\n    with torch.no_grad():\n        for inputs, _ in test_loader:  # Assuming test data has no labels\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            preds = (outputs.squeeze() > 0.5).cpu().numpy()  # For binary classification (adjust for multi-class)\n            predictions.extend(preds)\n\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:24:06.345753Z","iopub.status.idle":"2025-03-22T07:24:06.346328Z","shell.execute_reply":"2025-03-22T07:24:06.346063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load test data\ntest_dir = '/kaggle/input/bttai-ajl-2025/test/test/'\n# test_generator = preprocess_test_data(test_df, test_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:24:06.347228Z","iopub.status.idle":"2025-03-22T07:24:06.347749Z","shell.execute_reply":"2025-03-22T07:24:06.347547Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Generate Predictions","metadata":{}},{"cell_type":"code","source":"# TODO\n# Generate predictions based on the trained model\n# Then, save the predictions into a CSV file for submission\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n# Generate predictions for the test set\npredictions = preprocess_test_data(test_df, test_dir)\n\n# Save predictions to CSV\ntest_df['predictions'] = predictions\n(test_df.to_csv('predictions.csv', index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:24:06.348923Z","iopub.status.idle":"2025-03-22T07:24:06.349370Z","shell.execute_reply":"2025-03-22T07:24:06.349178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ncreate_download_link(test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T07:24:06.351758Z","iopub.status.idle":"2025-03-22T07:24:06.352260Z","shell.execute_reply":"2025-03-22T07:24:06.352059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}